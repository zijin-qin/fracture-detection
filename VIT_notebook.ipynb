{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_gf2jl91HTY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from matplotlib.image import imread\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import zipfile\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YkWaFxEJfz-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"New working directory:\", os.getcwd())#zip_path = 'drive/MyDrive/CSE 151A Final Project/FracAtlas.zip'\n",
        "zip_path = 'FracAtlas.zip'\n",
        "extract_dir = 'fracatlas_extracted'\n",
        "\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "images_dir = os.path.join(extract_dir, 'FracAtlas/images')\n",
        "annotations_dir = os.path.join(extract_dir, 'FracAtlas/Annotations/YOLO')\n",
        "# Checked to see if each image has a corresponding label\n",
        "\n",
        "categories = ['Fractured', 'Non_fractured']\n",
        "\n",
        "for cat in categories:\n",
        "    category_path = os.path.join(images_dir, cat)\n",
        "\n",
        "    for img_file in os.listdir(category_path):\n",
        "        if img_file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "            img_name = os.path.splitext(img_file)[0]\n",
        "            annotation_file = img_name + '.txt'\n",
        "            annotation_path = os.path.join(annotations_dir, annotation_file)"
      ],
      "metadata": {
        "id": "oTjOGnuLlY1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = 'fracatlas_extracted/FracAtlas/images'\n",
        "annotations_dir = 'fracatlas_extracted/FracAtlas/Annotations/YOLO'\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for category in os.listdir(images_dir):\n",
        "    category_path = os.path.join(images_dir, category)\n",
        "\n",
        "    if not os.path.isdir(category_path):\n",
        "        continue\n",
        "\n",
        "    for img_file in os.listdir(category_path):\n",
        "        if img_file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "            img_name = os.path.splitext(img_file)[0]\n",
        "            annotation_file = img_name + '.txt'\n",
        "            annotation_path = os.path.join(annotations_dir, annotation_file)\n",
        "\n",
        "            if os.path.exists(annotation_path):\n",
        "                img_path = os.path.join(category_path, img_file)\n",
        "                images.append(img_path)\n",
        "                labels.append(category)\n",
        "            else:\n",
        "                print(f\"No annotation found for {img_file} at {annotation_path}.\")\n",
        "\n",
        "data_dict_old = {\n",
        "    \"inputs\": images,\n",
        "    \"labels\": labels\n",
        "}\n",
        "\n",
        "print(f\"{len(images)} image paths and {len(labels)} labels in organized data\")"
      ],
      "metadata": {
        "id": "qN7UX8n2lxmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img_path):\n",
        "    try:\n",
        "        img = image.load_img(img_path, target_size=(224, 224))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = img_array / 255.0 # normalize pixel values\n",
        "        return img_array\n",
        "    except (OSError, IOError) as e:\n",
        "        warnings.warn(f\"Skipping corrupted image: {img_path}. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for img_path, label in zip(data_dict_old[\"inputs\"], data_dict_old[\"labels\"]):\n",
        "    img = preprocess(img_path)\n",
        "    if img is not None:\n",
        "        images.append(img)\n",
        "        labels.append(label)\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "data_dict = {\n",
        "    \"inputs\": images,\n",
        "    \"labels\": labels\n",
        "}\n",
        "\n",
        "print(f\"{len(images)}/{len(data_dict_old['inputs'])} images processed\")"
      ],
      "metadata": {
        "id": "0g_rlPAVm9Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(data_dict[\"labels\"])\n",
        "data_dict[\"encoded_labels\"] = encoded_labels\n",
        "# Function to convert image to grayscale\n",
        "def convert_to_grayscale(image):\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Function to normalize pixel values to [0, 1]\n",
        "def normalize(image):\n",
        "    return image / 255.0\n",
        "\n",
        "# Function to standardize the image data, mean=0 and std=1\n",
        "def standardize(image):\n",
        "    mean = np.mean(image)\n",
        "    std = np.std(image)\n",
        "    standardized_image = (image - mean) / std\n",
        "    return standardized_image\n",
        "# Convert images to grayscale and save\n",
        "def convert_and_save_grayscale_images(folder_path, save_folder, target_size=(224, 224)):\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)  # Make sure save folder exists\n",
        "\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    for i, filename in enumerate(image_files):\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Skip over corrupt images\n",
        "        if image is None:\n",
        "            warnings.warn(f\"Skipping corrupt or unreadable image: {image_path}\")\n",
        "            continue\n",
        "\n",
        "        image_resized = cv2.resize(image, target_size)\n",
        "\n",
        "        # Convert to grayscale\n",
        "        grayscale_image = convert_to_grayscale(image_resized)\n",
        "\n",
        "        # Save the grayscale image\n",
        "        processed_image_path = os.path.join(save_folder, f\"grayscale_{i}.npy\")\n",
        "        np.save(processed_image_path, grayscale_image)\n",
        "\n",
        "        # Free up memory\n",
        "        del image, grayscale_image\n",
        "\n",
        "    print(\"Grayscale conversion completed.\")\n",
        "\n",
        "# Normalize the grayscale images\n",
        "def normalize_and_save_images(folder_path, save_folder):\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.npy')]\n",
        "\n",
        "    for i, filename in enumerate(image_files):\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "        grayscale_image = np.load(image_path)\n",
        "\n",
        "        # Normalize the grayscale image\n",
        "        normalized_image = normalize(grayscale_image)\n",
        "\n",
        "        # Save the normalized image\n",
        "        processed_image_path = os.path.join(save_folder, f\"normalized_{i}.npy\")\n",
        "        np.save(processed_image_path, normalized_image)\n",
        "    print(\"Normalization completed.\")\n",
        "\n",
        "# Standardize the normalized images\n",
        "def standardize_and_save_images(folder_path, save_folder):\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.npy')]\n",
        "\n",
        "    for i, filename in enumerate(image_files):\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "        normalized_image = np.load(image_path)\n",
        "\n",
        "        # Standardize the normalized image\n",
        "        standardized_image = standardize(normalized_image)\n",
        "\n",
        "        # Save the standardized image\n",
        "        processed_image_path = os.path.join(save_folder, f\"standardized_{i}.npy\")\n",
        "        np.save(processed_image_path, standardized_image)\n",
        "\n",
        "        # Free up memory\n",
        "        del normalized_image, standardized_image\n",
        "\n",
        "    print(\"Standardization completed.\")"
      ],
      "metadata": {
        "id": "Azy0bWa3nCMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls fracatlas_extracted/FracAtlas/images/\n",
        "\n",
        "!ls 'fracatlas_extracted/FracAtlas/images/Fractured'"
      ],
      "metadata": {
        "id": "r94dJdYOnxxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frac_path = '/content/Processed/Final/Processed_Fractured_Final'\n",
        "unfrac_path = '/content/Processed/Final/Processed_Non_Fractured_Final'"
      ],
      "metadata": {
        "id": "x1BMiiAqSNOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fractured_path = 'fracatlas_extracted/FracAtlas/images/Fractured'\n",
        "non_fractured_path = 'fracatlas_extracted/FracAtlas/images/Non_fractured'\n",
        "\n",
        "# Folders to save the processed images for fractured and non-fractured\n",
        "grayscale_fractured_folder = 'Processed/Grayscale/Processed_Fractured_Grayscale'\n",
        "grayscale_non_fractured_folder = 'Processed/Grayscale/Processed_Non_Fractured_Grayscale'\n",
        "\n",
        "normalized_fractured_folder = 'Processed/Normalize/Processed_Fractured_Normalized'\n",
        "normalized_non_fractured_folder = 'Processed/Normalize/Processed_Non_Fractured_Normalized'\n",
        "\n",
        "standardized_fractured_folder = 'Processed/Final/Processed_Fractured_Final'\n",
        "standardized_non_fractured_folder = 'Processed/Final/Processed_Non_Fractured_Final'"
      ],
      "metadata": {
        "id": "qL3Bkm2Mo1a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all fractured and non-fractured images to grayscale and save\n",
        "convert_and_save_grayscale_images(fractured_path, grayscale_fractured_folder)\n",
        "convert_and_save_grayscale_images(non_fractured_path, grayscale_non_fractured_folder)\n",
        "\n",
        "# Normalize grayscale images for both fractured and non-fractured categories\n",
        "normalize_and_save_images(grayscale_fractured_folder, normalized_fractured_folder)\n",
        "normalize_and_save_images(grayscale_non_fractured_folder, normalized_non_fractured_folder)\n",
        "\n",
        "# Standardize normalized images for both fractured and non-fractured categories\n",
        "standardize_and_save_images(normalized_fractured_folder, standardized_fractured_folder)\n",
        "standardize_and_save_images(normalized_non_fractured_folder, standardized_non_fractured_folder)\n",
        "\n",
        "print(\"All preprocessing steps completed for both fractured and non-fractured images.\")"
      ],
      "metadata": {
        "id": "P76fbmFlo5ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load fractured and unfractured images into arrays\n",
        "fracArr = []\n",
        "unfracArr = []\n",
        "for file in os.listdir(frac_path):\n",
        "  if file.endswith(\".npy\"):\n",
        "    fracArr.append(np.load(os.path.join(frac_path, file)))\n",
        "for file in os.listdir(unfrac_path):\n",
        "  if file.endswith(\".npy\"):\n",
        "    unfracArr.append(np.load(os.path.join(unfrac_path, file)))"
      ],
      "metadata": {
        "id": "8JjjJ6S_Sjm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine images and true/false values into one array\n",
        "dataset = []\n",
        "for i in range(len(fracArr)):\n",
        "  dataset.append([fracArr[i], True])\n",
        "for i in range(len(unfracArr)):\n",
        "  dataset.append([unfracArr[i], False])"
      ],
      "metadata": {
        "id": "8qEF02buUPG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get X and y arrays from the dataset\n",
        "dataset = np.array(dataset, dtype=object)\n",
        "X = dataset[:,0]\n",
        "y = dataset[:,1].astype('bool')"
      ],
      "metadata": {
        "id": "xQIO_gJdVOL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle and split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=3**-2, random_state=42)"
      ],
      "metadata": {
        "id": "ZuLhmuJOWaTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# reshape array to use with smote\n",
        "shape_orig = X_train[0].shape\n",
        "X_train = np.array([np.array(val).flatten() for val in X_train])\n",
        "\n",
        "# oversample to fix class imbalance in the training set\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# built flatten array\n",
        "X_train = np.array([np.array(val).reshape(shape_orig) for val in X_train])"
      ],
      "metadata": {
        "id": "k0z22-aaXCSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten the other arrays\n",
        "X_val = np.array([np.array(val) for val in X_val])\n",
        "X_test = np.array([np.array(val) for val in X_test])\n",
        "y_train = np.array([np.array(val) for val in y_train])\n",
        "y_val = np.array([np.array(val) for val in y_val])\n",
        "y_test = np.array([np.array(val) for val in y_test])"
      ],
      "metadata": {
        "id": "T5tGjS1kb3To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "-2qjN786qO58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print some random examples from the training set\n",
        "arr = random.sample(range(0, len(X_train)), 3)\n",
        "for i in arr:\n",
        "  plt.imshow(X_train[i], cmap='gray')\n",
        "  plt.show()\n",
        "  print(\"Fracture present:\", y_train[i])"
      ],
      "metadata": {
        "id": "14HGTZSwTDIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to overlay bounding boxes on images\n",
        "def draw_bounding_boxes(img_path, annotation_path, save_dir=None):\n",
        "    # Loading the image\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img_height, img_width, _ = img.shape\n",
        "\n",
        "    # Checking if annotation exists\n",
        "    if not os.path.exists(annotation_path):\n",
        "        warnings.warn(f\"Annotation file {annotation_path} not found. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    # Reading YOLO annotations\n",
        "    with open(annotation_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "\n",
        "        # YOLO format: class_id, x_center, y_center, width, height (all relative)\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) != 5:\n",
        "            warnings.warn(f\"Invalid annotation format in {annotation_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        class_id, x_center, y_center, width, height = map(float, parts)\n",
        "\n",
        "        # Converting relative coordinates to absolute pixel values\n",
        "        x_center *= img_width\n",
        "        y_center *= img_height\n",
        "        width *= img_width\n",
        "        height *= img_height\n",
        "\n",
        "        # Calculating box corners\n",
        "        x_min = int(x_center - width / 2)\n",
        "        y_min = int(y_center - height / 2)\n",
        "        x_max = int(x_center + width / 2)\n",
        "        y_max = int(y_center + height / 2)\n",
        "\n",
        "        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        save_path = os.path.join(save_dir, os.path.basename(img_path))\n",
        "        plt.imsave(save_path, img)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "for img_path, label in zip(data_dict_old[\"inputs\"][:5], data_dict_old[\"labels\"][:5]):\n",
        "\n",
        "    img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "    annotation_path = os.path.join(annotations_dir, img_name + '.txt')\n",
        "\n",
        "    draw_bounding_boxes(img_path, annotation_path)"
      ],
      "metadata": {
        "id": "43JxlmKgBXXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras"
      ],
      "metadata": {
        "id": "wRbkxGxlflBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for the ViT\n",
        "PATCH_SIZE = 16\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "NUM_LAYERS = 4\n",
        "MLP_DIM = 256\n",
        "DROPOUT_RATE = 0.1\n",
        "INPUT_SHAPE = (224, 224, 1)\n",
        "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE) * (INPUT_SHAPE[1] // PATCH_SIZE)\n",
        "NUM_CLASSES = len(np.unique(y_train))\n",
        "\n",
        "# Define patch and encoding layers\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dim = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dim])\n",
        "        return patches\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
        "\n",
        "    def call(self, patches):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patches) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "# Build the Vision Transformer model\n",
        "def create_vit_model():\n",
        "    inputs = layers.Input(shape=INPUT_SHAPE)\n",
        "    patches = Patches(PATCH_SIZE)(inputs)\n",
        "    encoded_patches = PatchEncoder(NUM_PATCHES, D_MODEL)(patches)\n",
        "\n",
        "    for _ in range(NUM_LAYERS):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=NUM_HEADS, key_dim=D_MODEL, dropout=DROPOUT_RATE\n",
        "        )(x1, x1)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = layers.Dense(MLP_DIM, activation=tf.nn.gelu)(x3)\n",
        "        x3 = layers.Dropout(DROPOUT_RATE)(x3)\n",
        "        x3 = layers.Dense(D_MODEL)(x3)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.GlobalAveragePooling1D()(representation)\n",
        "    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(representation)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "vit_model = create_vit_model()\n",
        "vit_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "vit_model.summary()"
      ],
      "metadata": {
        "id": "zUTpT44KTVlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = vit_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        ")"
      ],
      "metadata": {
        "id": "LKkfRWOIvhPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "test_loss, test_accuracy = vit_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "_e0mQBe8wF39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yeh7o3qZeRr2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}